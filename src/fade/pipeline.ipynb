{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinstall the package\n",
    "%cd /Users/robertford/Repos/fade\n",
    "!source .venv/bin/activate\n",
    "%uv pip install -e .\n",
    "# or to be more specific\n",
    "# %uv pip install matplotlib scikit-image notebook numpy pandas pillow scipy tqdm ipykernel jupyter paddlepaddle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cell to verify installations\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Test core dependencies\n",
    "try:\n",
    "    import matplotlib\n",
    "    print(f\"matplotlib version: {matplotlib.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"matplotlib import error: {e}\")\n",
    "\n",
    "try:\n",
    "    import numpy\n",
    "    print(f\"numpy version: {numpy.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"numpy import error: {e}\")\n",
    "\n",
    "try:\n",
    "    import pandas\n",
    "    print(f\"pandas version: {pandas.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"pandas import error: {e}\")\n",
    "\n",
    "try:\n",
    "    import PIL\n",
    "    print(f\"Pillow version: {PIL.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"Pillow import error: {e}\")\n",
    "\n",
    "try:\n",
    "    import skimage\n",
    "    print(f\"scikit-image version: {skimage.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"scikit-image import error: {e}\")\n",
    "\n",
    "try:\n",
    "    import scipy\n",
    "    print(f\"scipy version: {scipy.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"scipy import error: {e}\")\n",
    "\n",
    "try:\n",
    "    import paddle\n",
    "    print(f\"paddle version: {paddle.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"paddle import error: {e}\")\n",
    "\n",
    "# Test FADE package\n",
    "try:\n",
    "    from fade.pipeline import PipelineState\n",
    "    print(\"FADE package imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"FADE package import error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025/03/19 16:52:59] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, use_gcu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/Users/robertford/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/Users/robertford/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/Users/robertford/Repos/fade/.venv/lib/python3.11/site-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/Users/robertford/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, onnx_providers=False, onnx_sess_options=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, formula_algorithm='LaTeXOCR', formula_model_dir=None, formula_char_dict_path=None, formula_batch_num=1, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, formula=False, ocr=True, recovery=False, recovery_to_markdown=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
      "PDF path: /Users/robertford/Repos/fade/data/f1040.pdf\n",
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "# Test the pipeline with a sample document\n",
    "from fade.pipeline import (\n",
    "    PipelineState,\n",
    "    setup_working_directory,\n",
    "    extract_document_pages,\n",
    "    detect_entities,\n",
    "    classify_entities,\n",
    "    report_unclassified_entities,\n",
    "    process_entities,\n",
    "    log_process\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "# Get absolute path to the PDF using the correct base path\n",
    "base_path = \"/Users/robertford/Repos/fade\"  # Your project root\n",
    "pdf_path = os.path.join(base_path, \"data\", \"f1040.pdf\")\n",
    "print(f\"PDF path: {pdf_path}\")\n",
    "print(f\"File exists: {os.path.exists(pdf_path)}\")\n",
    "\n",
    "# Initialize a test state with absolute path\n",
    "initial_state = PipelineState(\n",
    "    document_id=pdf_path,  # Using absolute path\n",
    "    working_dir=\"\",\n",
    "    images=[],\n",
    "    entities={},\n",
    "    unclassified_entities={},\n",
    "    logs=[],\n",
    "    error=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FADE Pipeline Steps\n",
    "\n",
    "#### 1. Setup Working Directory\n",
    "Creates a working directory for processing the document, copying the source file to a dedicated workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing pipeline setup...\n",
      "\n",
      "Setup Working Directory Logs:\n",
      "- Created working directory: /Users/robertford/Repos/fade/data/f1040.pdf_working\n"
     ]
    }
   ],
   "source": [
    "# Setup working directory\n",
    "print(\"\\nTesting pipeline setup...\")\n",
    "state = setup_working_directory(initial_state)\n",
    "print(\"\\nSetup Working Directory Logs:\")\n",
    "for log in state.logs:\n",
    "    print(f\"- {log['message']}\")\n",
    "if state.error:\n",
    "    print(f\"Error: {state.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Extract Document Pages\n",
    "Converts each page of the PDF into an image file. Also extracts and processes any embedded PDF attachments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing document page extraction...\n",
      "Error processing attachments in f1040.pdf: 'Document' object has no attribute 'embfile_get_names'\n",
      "\n",
      "Extract Document Pages Logs:\n",
      "- Created working directory: /Users/robertford/Repos/fade/data/f1040.pdf_working\n",
      "- Extracted 2 page images\n",
      "\n",
      "Number of images extracted: 2\n"
     ]
    }
   ],
   "source": [
    "# Test extract document pages\n",
    "print(\"\\nTesting document page extraction...\")\n",
    "state = extract_document_pages(state)\n",
    "print(\"\\nExtract Document Pages Logs:\")\n",
    "for log in state.logs:\n",
    "    print(f\"- {log['message']}\")\n",
    "print(f\"\\nNumber of images extracted: {len(state.images)}\")\n",
    "if state.error:\n",
    "    print(f\"Error: {state.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Detect Entities\n",
    "Uses PaddleOCR to detect and extract text elements from each page image. Creates visualizations showing detected text regions.\n",
    "Key optimizations:\n",
    "- Multiprocessing enabled\n",
    "- Image size limited to 960px\n",
    "- Fast recognition model (SVTR_LCNet)## 3. Detect Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing entity detection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pages:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Test detect entities\n",
    "if len(state.images) > 0:\n",
    "    print(\"\\nTesting entity detection...\")\n",
    "    state = detect_entities(state)\n",
    "    print(\"\\nDetect Entities Logs:\")\n",
    "    for log in state.logs:\n",
    "        print(f\"- {log['message']}\")\n",
    "    print(f\"\\nNumber of entities detected: {len(state.entities)}\")\n",
    "    if state.error:\n",
    "        print(f\"Error: {state.error}\")\n",
    "else:\n",
    "    print(\"\\nSkipping entity detection since no images were extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Classify Entities\n",
    "Categorizes detected entities into types (text, data, image, etc.) using layout detection and OCR confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test classify_entities\n",
    "print(\"\\nTesting entity classification...\")\n",
    "state = classify_entities(state)\n",
    "print(\"\\nClassify Entities Logs:\")\n",
    "for log in state.logs:\n",
    "    print(f\"- {log['message']}\")\n",
    "print(f\"\\nClassified entities: {len(state.entities) - len(state.unclassified_entities)}\")\n",
    "print(f\"Unclassified entities: {len(state.unclassified_entities)}\")\n",
    "if state.error:\n",
    "    print(f\"Error: {state.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Report Unclassified Entities\n",
    "Generates a report of any entities that couldn't be automatically classified, allowing for manual review if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test report_unclassified_entities\n",
    "print(\"\\nTesting unclassified entities report...\")\n",
    "state = report_unclassified_entities(state)\n",
    "print(\"\\nUnclassified Entities Report Logs:\")\n",
    "for log in state.logs:\n",
    "    print(f\"- {log['message']}\")\n",
    "if state.error:\n",
    "    print(f\"Error: {state.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Process Entities\n",
    "Processes each classified entity according to its type:\n",
    "- Text: Extracts content using OCR\n",
    "- Data: Converts tables to CSV\n",
    "- Images: Saves as PNG files\n",
    "Creates a JSON structure documenting all entities and their relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test process_entities\n",
    "print(\"\\nTesting entity processing...\")\n",
    "state = process_entities(state)\n",
    "print(\"\\nProcess Entities Logs:\")\n",
    "for log in state.logs:\n",
    "    print(f\"- {log['message']}\")\n",
    "if state.error:\n",
    "    print(f\"Error: {state.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Log Process\n",
    "Records the entire processing pipeline's results, including:\n",
    "- Processing times\n",
    "- Success/failure rates\n",
    "- Entity counts\n",
    "- Error logs\n",
    "Saves this information for analysis and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test log_process\n",
    "print(\"\\nTesting process logging...\")\n",
    "state = log_process(state)\n",
    "print(\"\\nLog Process Logs:\")\n",
    "for log in state.logs:\n",
    "    print(f\"- {log['message']}\")\n",
    "if state.error:\n",
    "    print(f\"Error: {state.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final State Summary\n",
    "Shows the complete processing results:\n",
    "- Document information\n",
    "- Entity counts\n",
    "- Output files generated\n",
    "- Any errors encountered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final state summary\n",
    "print(\"\\nFinal State Summary:\")\n",
    "print(f\"Document ID: {state.document_id}\")\n",
    "print(f\"Working Directory: {state.working_dir}\")\n",
    "print(f\"Number of Images: {len(state.images)}\")\n",
    "print(f\"Number of Entities: {len(state.entities)}\")\n",
    "print(f\"Number of Unclassified Entities: {len(state.unclassified_entities)}\")\n",
    "print(f\"Error: {state.error}\")\n",
    "\n",
    "# Print output directory contents\n",
    "import os\n",
    "output_dir = os.path.join(state.working_dir, \"output\")\n",
    "if os.path.exists(output_dir):\n",
    "    print(\"\\nOutput Directory Contents:\")\n",
    "    for item in os.listdir(output_dir):\n",
    "        print(f\"- {item}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
